---
title: "Met_Diet_129"
author: "Qi Yan"
date: "8/21/2019"
output: 
  html_document:
    toc: TRUE # table of content true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    toc_float: true
    number_sections: FALSE
    theme: united  
    highlight: tango
    df_print: paged
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(xlsx)
library(doParallel)
library(ggridges)
library(ggplot2)
library(viridis)
library(hrbrthemes)
library(gridExtra)
library(factoextra)
library(ggpubr)

options(stringsAsFactors = FALSE)
```

# Data Preprocessing

## Import Data

```{r import data, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
setwd("/Users/qiyan/Dropbox/PARENTs")
met_raw <- read.xlsx(file = "/Users/qiyan/Dropbox/PARENTs/Raw Data/all_mets_corrected_exclude_where_std_low.xlsx", sheetName = "Corrected", header = T)
met_raw <- met_raw[which(met_raw$C_Label == 0), -2]
met_raw <- met_raw[,order(colnames(met_raw))]

key <- read.xlsx(file = "/Users/qiyan/Dropbox/PARENTs/Raw Data/PARENTs Sample Guide.xlsx", sheetIndex = 2, header = T)
met_class <- read.xlsx(file = "/Users/qiyan/Dropbox/PARENTs/Raw Data/Metabolite_list_category.xlsx", sheetIndex = 1, header = T)
pheno_raw <- read.csv(file = "/Users/qiyan/Dropbox/PARENTs/Raw Data/PARENTs_KeyVariables_190403.csv", header = T)
nut_raw <- read.xlsx(file = "/Users/qiyan/Dropbox/PARENTs/Raw Data/dietcalc_results_20190621.results_IDR.xlsx", sheetIndex = 1, header = T)

# Link Christofk_ID with Subject_ID
temp_ID <- data.frame(colnames(met_raw)[-1]); colnames(temp_ID) <- "Christofk_ID"
temp_ID <- merge(temp_ID, key, by = "Christofk_ID")
colnames(met_raw) <- c("Compound", temp_ID$Subject_ID); rownames(met_raw) <- met_raw$Compound

# Create clean datasets for downstream analyses
met <- data.frame(t(met_raw[,-1])); met <- cbind(rownames(met), met)
colnames(met)[1] <- "Subject_ID"

pheno <- pheno_raw[which(pheno_raw$Subject_ID %in% met$Subject_ID),] # PJR81X, PIM84B, PDD77V couldn't find a match

nut <- nut_raw[which(nut_raw$PARENTs.ID %in% met$Subject_ID),]

save(met, pheno, nut, met_class, file = "/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_raw.RData")
```

```{r , echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
VennDiagram::draw.triple.venn(area1 = 22, area2 = 20, area3 = 13, n12 = 11, n23 = 4, n13 = 5, 
    n123 = 1, category = c("Dog People", "Cat People", "Lizard People"), lty = "blank", 
    fill = c("skyblue", "pink1", "mediumorchid"))
```
![Venn Diagram of sample sizes](/Users/qiyan/Dropbox/PARENTs/Output/Venn_samplesize.PNG)

## Quality Control

Here is Ernst's reply: 

So original: the raw peak areas of metabolites and their isotopomers outputed by the programs no normalization or anything;

corrected: the peak areas of the isotopomers for each metabolite corrected for the natural abundance of C13 ;

PoolAfterDF: the sum of all the peak areas across all the isotopomers for a specific metabolite;

Trifluoromethanosulfate is the standard I added. The same amount is added to all samples so it should have the same peak area. It is not necessary but can be helpeful to normalize to it so that you reduce the effects of machine performance drift throughout the runs.

### Distribution of raw intensity

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 15, fig.width = 15}
load("/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_raw.RData")
met_raw_stat <- data.frame(colnames(met[,-1])); colnames(met_raw_stat)[1] <- "metabolite"
met_raw_stat$mean <- apply(met[,-1], 2, mean); met_raw_stat$sd <- apply(met[,-1], 2, sd)
met_raw_stat$rsd <- met_raw_stat$sd/met_raw_stat$mean
met_raw_stat <- met_raw_stat[order(-met_raw_stat$rsd),]
met_raw_stat[1:10,]
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 15, fig.width = 15}
ggplot(stack(met[,-1]), aes(x = values, y = ind, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_fill_viridis(option = "C") +
  labs(title = 'Distribution of the raw intensity of metabolites', x = "Intensity", y = "Metabolites") +
  theme_ipsum() +
  theme(
    legend.position="none",
    panel.spacing = unit(0.1, "lines"),
    strip.text.x = element_text(size = 8)
  )

ggplot(stack(subset(met, select = -c(Subject_ID, trifluoromethanosulfate))), aes(x = values, y = ind, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_fill_viridis(option = "C") +
  labs(title = 'Distribution of the raw intensity of metabolites \n w/o trifluoromethanosulfate', x = "Intensity", y = "Metabolites") +
  theme_ipsum() +
  theme(
    legend.position="none",
    panel.spacing = unit(0.1, "lines"),
    strip.text.x = element_text(size = 8)
  )
```

### Missing value

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
met_missing <- data.frame(colnames(met[,-1]))
met_missing$num_missing <- apply(met[,-1], 2, function(x) sum(x==0))
met_missing$percent_missing <- round(met_missing$num_missing/129*100,2)
colnames(met_missing)[1] <- "metabolite"
met_missing <- met_missing[order(-met_missing$num_missing),]
met_missing[which(met_missing$num_missing!=0),]
```

Impute missing value (0) with half of the minimum positive value in the original data.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
met_impute <- met; met_impute[met_impute==0] <- NA
halfdatmin <- min(met_impute[,-1], na.rm = TRUE)*0.5
met_impute[is.na(met_impute)] <- halfdatmin
```

### Transformation

Conduct generalized logarithm transformation.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
LogNorm<-function(x, min.val){
  log2((x + sqrt(x^2 + min.val^2))/2)
}
min.val <- halfdatmin/5
met_norm <- data.frame(cbind(met_impute$Subject_ID, apply(met_impute[,-1], 2, LogNorm, min.val))); colnames(met_norm)[1] <- "Subject_ID"
met_norm[,-1] <- apply(met_norm[,-1], 2, function(x) as.numeric(x))
```

### Batch Effect

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
batch <- read.xlsx(file = "/Users/qiyan/Dropbox/PARENTs/Input/met_129_metaboanalyst.xlsx", sheetIndex = 1, header = T); batch <- batch[,1:2]
batch_impute <- merge(batch, met_norm, by = "Subject_ID")
batch_pca <- prcomp(batch_impute[,3:ncol(batch_impute)], center = TRUE, scale. = TRUE)

ggbiplot::ggbiplot(batch_pca, ellipse=FALSE,  groups=batch_impute$Batch)
```

Correlation between top 5 PCs and batchs.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
batch_cor <- data.frame(PCs = colnames(batch_pca$x[,1:5]))
batch_cor$corr <- apply(batch_pca$x[,1:5], 2, function(x) cor.test(x, as.numeric(as.factor(batch_impute$Batch)))$estimate)
batch_cor$P_value <- apply(batch_pca$x[,1:5], 2, function(x) cor.test(x, as.numeric(as.factor(batch_impute$Batch)))$p.value)

batch_cor
```

Control for batch effect using combat.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
met_trans <- subset(met_norm, select = -c(trifluoromethanosulfate))
# batch <- read.xlsx(file = "/Users/qiyan/Dropbox/PARENTs/Input/met_129_metaboanalyst.xlsx", sheetIndex = 1, header = T); batch <- batch[,1:2]
met_trans <- merge(batch, met_trans, by = "Subject_ID")
rownames(met_trans) <- met_trans$Subject_ID

met_trans <- sva::ComBat(dat=t(met_trans[,3:ncol(met_trans)]), batch=met_trans$Batch, mod=NULL, par.prior=TRUE, prior.plots=FALSE)
met_trans <- data.frame(colnames(met_trans), t(met_trans)); colnames(met_trans)[1] <- "Subject_ID"

batch_combat <- merge(batch, met_trans, by = "Subject_ID")
batch_pca <- prcomp(batch_combat[,3:ncol(batch_combat)], center = T, scale. = T)
ggbiplot::ggbiplot(batch_pca, ellipse=FALSE,  groups=batch_combat$Batch, var.axes = F)

batch_cor <- data.frame(PCs = colnames(batch_pca$x[,1:5]))
batch_cor$corr <- apply(batch_pca$x[,1:5], 2, function(x) cor.test(x, as.numeric(as.factor(batch_combat$Batch)))$estimate)
batch_cor$P_value <- apply(batch_pca$x[,1:5], 2, function(x) cor.test(x, as.numeric(as.factor(batch_combat$Batch)))$p.value)

batch_cor
```

Control for batch effect by normalization by reference feature trifluoromethanosulfate.

This method didn't work well. Couldn't fully adjust for batch effect, and highly correlated data.

```{r , echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
CompNorm<-function(x, ref){
  10E9*x/x[ref];
}

met_trans<-t(apply(met_norm[,-1], 1, CompNorm, "trifluoromethanosulfate"))
met_trans <- data.frame(met_norm$Subject_ID, met_trans); colnames(met_trans)[1] <- "Subject_ID"
met_trans <- subset(met_trans, select = -c(trifluoromethanosulfate))

# batch <- read.xlsx(file = "/Users/qiyan/Dropbox/PARENTs/Input/met_129_metaboanalyst.xlsx", sheetIndex = 1, header = T); batch <- batch[,1:2]
batch_ref <- merge(batch, met_trans, by = "Subject_ID")
batch_pca <- prcomp(batch_ref[,3:ncol(batch_ref)], center = F, scale. = F)

batch_cor <- data.frame(PCs = colnames(batch_pca$x[,1:5]))
batch_cor$corr <- apply(batch_pca$x[,1:5], 2, function(x) cor.test(x, as.numeric(as.factor(batch_ref$Batch)))$estimate)
batch_cor$P_value <- apply(batch_pca$x[,1:5], 2, function(x) cor.test(x, as.numeric(as.factor(batch_ref$Batch)))$p.value)

batch_cor
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 15, fig.width = 15}
ggplot(stack(met_trans[,-1]), aes(x = values, y = ind, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_fill_viridis(option = "C") +
  labs(title = 'Distribution of the raw intensity of metabolites', x = "Intensity", y = "Metabolites") +
  theme_ipsum() +
  theme(
    legend.position="none",
    panel.spacing = unit(0.1, "lines"),
    strip.text.x = element_text(size = 8)
  )
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Save data: met_norm - log transformation; met_trans - log transformed and correct for batch
save(met, met_norm, met_trans, pheno, nut, met_class, file = "/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_trans.RData")
```

# Data Mining

## Metabolites

### Correlation between metabolites

```{r , echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
load("/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_trans.RData")

met_corr <- met_trans[,-1]
met_corr <- met_corr[,order(colnames(met_corr))]

met_corr <- cor(met_corr)
annotation_col <- met_class[which(met_class$Met_ID %in% colnames(met_corr)), -2]; annotation_col <- annotation_col[order(annotation_col$Met_ID),]
rownames(annotation_col) <- annotation_col$Met_ID; annotation_col <- subset(annotation_col, select = -c(Met_ID))

colors <- rev(colorRampPalette(RColorBrewer::brewer.pal(10, "RdBu"))(256))
pheatmap::pheatmap(mat = met_corr, fontsize=12, fontsize_row=12, color = colors, annotation_col = annotation_col)
```

![Heatmap of correlation between metabolites](/Users/qiyan/Dropbox/PARENTs/Output/heatmap_metabolites.PNG)

## Metabolites - Phenotypes

Only 126 subjects have pheno data, so restricted to 126 subjects.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
rm(list = ls())
load("/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_trans.RData")
met_pheno_input <- met_trans[which(rownames(met_trans) %in% pheno$Subject_ID), -1]
met_pheno_input <- met_pheno_input[,order(colnames(met_pheno_input))]
rownames(pheno) <- pheno$Subject_ID
# met_pheno_input_untrans <- met_norm[which(rownames(met_norm) %in% pheno$Subject_ID), -1]
# met_pheno_input_untrans <- met_pheno_input_untrans[order(rownames(met_pheno_input_untrans)),order(colnames(met_pheno_input_untrans))]

# split pheno dataset into numeric and character ones
pheno_numeric <- select_if(pheno, is.numeric)
pheno_char <- select_if(pheno, is.character)
```

### PCA analysis

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
pca <- prcomp(met_pheno_input, center = T, scale. = T)
pca_shiny <- pca # for shiny use... avoid being rewritten by other codes
summary(pca)
pca_score <- data.frame(pca$x[,1:10])

ggbiplot::ggbiplot(pca, ellipse=FALSE, var.axes = T) +
  labs(title = "PCA plot of metabolites")
```

Plot the loadings of PCs.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 12, fig.width = 12}
pca_loading <- pca$rotation

annotation_col <- met_class[which(met_class$Met_ID %in% rownames(pca_loading)), -2]; annotation_col <- annotation_col[order(annotation_col$Met_ID),]
rownames(annotation_col) <- annotation_col$Met_ID; annotation_col <- subset(annotation_col, select = -c(Met_ID))

colors <- rev(colorRampPalette(RColorBrewer::brewer.pal(10, "RdBu"))(256))
pheatmap::pheatmap(mat = pca_loading[,1:10], fontsize=12, fontsize_row=12, color = colors, annotation_row = annotation_col, cluster_cols = F, cellwidth = 25, cellheight = 12, main = "Loadings"
)
```

#### ANOVA test: PC scores with categorical variables

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
anova <- data.frame(pca_score, pheno_char)
anova[anova == ""] <- NA
anova <- anova[,-c(11,12,18:22,29,30)]

anova_summary <- data.frame(colnames(anova), PC1 = 1, PC2 = 1, PC3 = 1, PC4 = 1, PC5 = 1, PC6 = 1, PC7 = 1, PC8 = 1, PC9 = 1, PC10 = 1)

for (i in 1:10) {
  for (j in c(11:ncol(anova))){
    anova_summary[j,i+1] <- summary(aov(anova[,i] ~ anova[,j], data = anova, na.action = na.exclude))[[1]][["Pr(>F)"]][1]
  }
}

anova_summary <- anova_summary[-c(1:10),]

significant_variable_cat <- {}
for (i in 1:nrow(anova_summary)) {
  if(sum(anova_summary[i,] < 0.05) > 0){
    temp <- anova_summary[i,1]
    significant_variable_cat <- cbind(significant_variable_cat, temp)
  }
}

print("Categorical variables significanly associated with PC1 - PC10")
as.vector(significant_variable_cat)
```

```{r, echo=FALSE}
library(shiny)

pheno_char_shiny <- anova[,-c(1:10)]

shinyApp(

  ui = fluidPage(
    selectInput("cov", "Categorical variable:",
                choices = colnames(pheno_char_shiny)),
    plotOutput("pcaPlot")
  ),

  server = function(input, output) {
    output$pcaPlot = renderPlot({
      ggbiplot::ggbiplot(pca_shiny, ellipse=FALSE, var.axes = F, groups = pheno_char_shiny[,input$cov]) +
  labs(title = "PCA plot of metabolites")
    })
  },

  options = list(height = 500)
)
```

#### Correlation test: PC scores with numeric variables

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
corr <- pheno_numeric[,-c(24:28)]

corr_summary <- data.frame(colnames(corr), PC1 = 1, PC2 = 1, PC3 = 1, PC4 = 1, PC5 = 1, PC6 = 1, PC7 = 1, PC8 = 1, PC9 = 1, PC10 = 1)

for (i in 1:10) {
  for (j in c(1:ncol(corr))){
    corr_summary[j,i+1] <- cor.test(corr[,j], pca_score[,i], na.action = na.rm)$p.value
  }
}


significant_variable_num <- {}
for (i in 1:nrow(corr_summary)) {
  if(sum(corr_summary[i,] < 0.05) > 0){
    temp <- corr_summary[i,1]
    significant_variable_num <- cbind(significant_variable_num, temp)
  }
}

print("Numeric variables significanly associated with PC1 - PC10")
as.vector(significant_variable_num)
```

```{r, echo=FALSE}
library(shiny)

pheno_num_shiny <- corr

shinyApp(
  
  ui = fluidPage(
    selectInput("cov", "Numeric variable:",
                choices = colnames(pheno_num_shiny)),
    column(6,
           plotOutput("histPlot"),
           verbatimTextOutput("text"))
  ),
  
  server = function(input, output) {
    output$histPlot = renderPlot({
      ggplot(pheno_num_shiny, aes(x=pheno_num_shiny[,input$cov],)) + 
        geom_histogram(colour = "black", fill = "lightseagreen") + 
        geom_rug(aes(y=0),  sides="b", position = "jitter") +
        theme(axis.title.x=element_blank())
    })
    output$text <- renderText({
      paste("Number of missing:", sum(is.na(pheno_num_shiny[,input$cov])))
      paste("Number of missing:", sum(is.na(pheno_num_shiny[,input$cov])), "\nMean:", mean(pheno_num_shiny[,input$cov], na.rm = T), "\nMedian:", median(pheno_num_shiny[,input$cov], na.rm = T))
    })
  },
  
  options = list(height = 500)
)
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 6}
num_corr <- cor(corr, pca_score,use = "p")

colors <- rev(colorRampPalette(RColorBrewer::brewer.pal(10, "RdBu"))(256))
pheatmap::pheatmap(mat = num_corr, fontsize=12, fontsize_row=12, color = colors, cluster_cols = F, cellheight = 10)
```

### PhenoMWAS

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
pheno_used <- pheno
rownames(pheno_used) <- pheno_used$Subject_ID
pheno_used <- pheno_used[,which(colnames(pheno_used) %in% c(corr_summary$colnames.corr., anova_summary$colnames.anova.))]
pheno_used[pheno_used == ""] <- NA

PhenoMWAS_summary <- data.frame(); k=1

for (i in 1:ncol(met_pheno_input)) {
  for (j in 1:ncol(pheno_used)) {
    temp <- summary(lm(met_pheno_input[,i] ~ pheno_used[,j]))$coefficients[-1,4]
    if(sum(temp < 0.05) > 0){
      PhenoMWAS_summary[k,1] <- colnames(pheno_used)[j]
      PhenoMWAS_summary[k,2] <- colnames(met_pheno_input)[i]
      k = k + 1
    }
  }
}

colnames(PhenoMWAS_summary) <- c("Pheno", "Metabolite")

PhenoMWAS_summary <- merge(x = PhenoMWAS_summary, y = met_class[,c(1,3)], by.x = "Metabolite", by.y = "Met_ID", all.x = T)
PhenoMWAS_summary <- PhenoMWAS_summary[order(PhenoMWAS_summary$Pheno),]
PhenoMWAS_summary <- PhenoMWAS_summary[,c("Pheno", "Metabolite", "Class")]

PhenoMWAS_summary

unique(PhenoMWAS_summary$Pheno)
unique(PhenoMWAS_summary$Metabolite)
```

## Diets

Only 94 subjects have diet data.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# rm(list = ls())
load("/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_trans.RData")

nut_used <- nut[,-c(1:6)]
rownames(nut_used) <- nut$PARENTs.ID

print("Number of diets:")
print(dim(nut_used))

head(nut_used)
```

### Missing values

Remove diet variables if everyone = 0;

Check diet variables if perentage of missing >= 0.8.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
print("Variables that everyone is mssing:"); colnames(nut_used[,which(apply(nut_used, 2, function(x) sum(x == 0))/94*100 == 100)])
# Remove 100% missing variables
nut_rmMissing <- nut_used[,-which(apply(nut_used, 2, function(x) sum(x == 0))/94*100 == 100)]

missing <- nut_used[,which(apply(nut_used, 2, function(x) sum(x == 0))/94*100 > 80 & apply(nut_used, 2, function(x) sum(x == 0))/94*100 < 100)]
print("Variable with more than 80% missing:"); colnames(missing)
missing
```

### Distribution and outliers

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 40, fig.width = 15}
ggplot(stack(nut_rmMissing), aes(x = values, y = ind, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_fill_viridis(option = "C") +
  labs(title = 'Distribution of the raw values of diet variables', x = "Values", y = "Diets") +
  theme_ipsum() +
  theme(
    legend.position="none",
    panel.spacing = unit(0.1, "lines"),
    strip.text.x = element_text(size = 8)
  )
```

Use __3SD__ as the threshold of outlier. Create a PDF report.

Notice: for all plots, zero values have been removed.

```{r , echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
pdf(file = "/Users/qiyan/Dropbox/PARENTs/Output/Diet_data_dist.pdf")

# First remove 0 values, and then check for outlier
# Inspired by https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/

data_expo <- function(dt) {
  for (i in 1:ncol(dt)) {
    var_name <- dt[,i]
    na1 <- sum(is.na(var_name))
    # check for 0
    var_missing <- sum(var_name == 0)
    var_name[which(var_name == 0)] <- NA # change 0 to NA
    # mean and sd with outliers
    m1 <- mean(var_name, na.rm = T)
    sd1 <- sd(var_name, na.rm = T)
    par(mfrow=c(2, 2), oma=c(0,0,5,0))
    boxplot(var_name, main="With outliers")
    hist(var_name, main="With outliers", xlab=NA, ylab=NA)
    # Detect outlier
    {
      lowerq = quantile(var_name, na.rm = T)[2]
      upperq = quantile(var_name, na.rm = T)[4]
      iqr = IQR(var_name, , na.rm = T)#Or use IQR(var_name)
      # we identify extreme outliers
      extreme.threshold.upper = (iqr * 3) + upperq
      extreme.threshold.lower = lowerq - (iqr * 3)
      outlier <- which(var_name > extreme.threshold.upper | var_name < extreme.threshold.lower)
      number_outlier <- length(outlier)
    }
    # mean of outlier
    mo <- mean(var_name[outlier])
    var_name[outlier] <- NA
    boxplot(var_name, main="Without outliers")
    hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
    m2 <- mean(var_name, na.rm = T)
    
    title_text <- paste(colnames(dt)[i], "\nNumber of 0s:", var_missing,"\nMean:", round(m1, 2), " SD:", round(sd1, 2), 
                        " Outliers identified:", number_outlier, 
                        "\nMean of the outliers:", round(mo, 2), " Mean if we remove outliers:", round(m2, 2))
    title(title_text, outer=TRUE, line = -1)
  }
}

data_expo(nut_rmMissing)
dev.off()
```

Check how many outliers per subject.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
count_outlier <- function(var_name){
  lowerq = quantile(var_name, na.rm = T)[2]
  upperq = quantile(var_name, na.rm = T)[4]
  iqr = IQR(var_name, , na.rm = T)#Or use IQR(var_name)
  # we identify extreme outliers
  extreme.threshold.upper = (iqr * 3) + upperq
  extreme.threshold.lower = lowerq - (iqr * 3)
  return(var_name > extreme.threshold.upper | var_name < extreme.threshold.lower)
}

missing_matrix <- apply(nut_rmMissing, 2, function(x) count_outlier(x))
rownames(missing_matrix) <- rownames(nut_rmMissing)

print("Subjects with more than 20% outliers:")
names(which(apply(missing_matrix, 1, function(x) sum(x)/dim(nut_rmMissing)[2]*100 > 20)))

print(sum(missing_matrix['PAB81P',])/dim(nut_rmMissing)[2]*100)
print(sum(missing_matrix['PLC79F',])/dim(nut_rmMissing)[2]*100)
```

Highlighted all outliers and exported to Excel.

```{r , echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Inspired by https://stackoverflow.com/questions/21618556/export-data-frames-to-excel-via-xlsx-with-conditional-formatting

indi <- "test"
for (i in 1:ncol(missing_matrix)) {
  if(sum(missing_matrix[,i]) > 0){
    row.indi <- which(missing_matrix[,i] == TRUE) + 1
    indi_temp <- paste(row.indi, ".", i+1, sep = "")
    indi <- c(indi, indi_temp)
  }
}
indi <- indi[-1]

# exporting data.frame to excel is easy with xlsx package
  sheetname <- "mysheet"
  write.xlsx(nut_rmMissing, "/Users/qiyan/Dropbox/PARENTs/Output/Diet_data_visual.xlsx", sheetName=sheetname)
  file <- "/Users/qiyan/Dropbox/PARENTs/Output/Diet_data_visual.xlsx"
# but we want to highlight cells if value greater than or equal to 5
  wb <- loadWorkbook(file)              # load workbook
  fo <- Fill(foregroundColor="yellow")  # create fill object
  cs <- CellStyle(wb, fill=fo)          # create cell style
  sheets <- getSheets(wb)               # get all sheets
  sheet <- sheets[[sheetname]]          # get specific sheet
  rows <- getRows(sheet, rowIndex=2:(nrow(nut_rmMissing)+1))     # get rows
                                                         # 1st row is headers
  cells <- getCells(rows, colIndex = 2:(ncol(nut_rmMissing)+1))       # get cells
# in the wb I import with loadWorkbook, numeric data starts in column 3
# and the first two columns are row number and label number

  values <- lapply(cells, getCellValue) # extract the values

lapply(names(cells[indi]),
       function(ii)setCellStyle(cells[[ii]],cs))

saveWorkbook(wb, file)
```

### Transformation and standardization

Different normalization method, how to find the correct one?

Right now decided to do log transform and then scale all vsriables to [0,1]. See this: https://www.datacamp.com/community/tutorials/hierarchical-clustering-R

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 40, fig.width = 15}
LogNorm<-function(x, min.val){
  log2((x + sqrt(x^2 + min.val^2))/2)
}
min.val <- 0.01/5
nut_trans <- data.frame(apply(nut_rmMissing, 2, LogNorm, min.val))

standardize <- function(x){(x-min(x))/(max(x)-min(x))}
nut_trans <- data.frame(apply(nut_trans, 2, function(x) standardize(x)))

head(nut_trans)

ggplot(stack(nut_trans), aes(x = values, y = ind, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_fill_viridis(option = "C") +
  labs(title = 'Distribution of the raw values of diet variables', x = "Values", y = "Diets") +
  theme_ipsum() +
  theme(
    legend.position="none",
    panel.spacing = unit(0.1, "lines"),
    strip.text.x = element_text(size = 8)
  )

# Save data: met_norm - correct for batches, but not log transformation; met_trans - log transformation; nut_rmMissing - removed all missing diet var; nut_trans - transformed diet data
save(met, met_norm, met_trans, pheno, nut, met_class, nut_rmMissing, nut_trans, file = "/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_trans.RData")
```

### Correlation and clustering analysis

__Correlation between diet variables__

```{r , echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE, fig.height = 40, fig.width = 15}
load(file = "/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_trans.RData")

nut_corr <- cor(nut_trans)
colors <- rev(colorRampPalette(RColorBrewer::brewer.pal(10, "RdBu"))(256))
pheatmap::pheatmap(mat = nut_corr, fontsize=6, fontsize_row=6, color = colors, cellwidth = 5.5, cellheight = 3, border_color=FALSE, show_rownames=F, legend = F)
```

![Heatmap of thr correlation between diet variables](/Users/qiyan/Dropbox/PARENTs/Output/heatmap_diet_correlation.PNG)

__Hierarchical Clustering__ based on https://uc-r.github.io/hc_clustering.

__Clustering of diet variables__

Choice of clustering methods: https://stats.stackexchange.com/questions/195456/how-to-select-a-clustering-method-how-to-validate-a-cluster-solution-to-warran/195481#195481; https://stats.stackexchange.com/questions/195446/choosing-the-right-linkage-method-for-hierarchical-clustering.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 20, fig.width = 30}
# rm(list = ls())
load(file = "/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_trans.RData")
nut_trans_dietCluster <- t(nut_trans)

# Dissimilarity matrix
d <- dist(nut_trans_dietCluster, method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )
# Plot the obtained dendrogram
# plot(as.dendrogram(hc1), cex = 2, hang = -1, main = "Cluster of subjects based on their diets (complete method)", horiz = TRUE)
plot(hc1, cex = 1, hang = -1, main = "Cluster of diets (complete method)", cex.main=3)

{
  library(cluster)
  # methods to assess agglomerative coefficient, which measures the amount of clustering structure found
  m <- c( "average", "single", "complete", "ward")
  names(m) <- c( "average", "single", "complete", "ward")
  
  # function to compute coefficient
  ac <- function(x) {
    agnes(nut_trans_dietCluster, method = x)$ac
  }
  map_dbl(m, ac)
}

hc2 <- hclust(d, method = "ward.D2" )
# plot(as.dendrogram(hc2), cex = 2, hang = -1, main = "Cluster of subjects based on their diets (ward D2 method)", horiz = TRUE)
plot(hc2, cex = 1, hang = -1, main = "Cluster of diets (ward D2 method)", cex.main=3)
```

Use ward.D2 method, and then cut trees.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Decide how many subgroups based on the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square) is minimized
# See thos post: https://uc-r.github.io/kmeans_clustering#silo
factoextra::fviz_nbclust(nut_trans_dietCluster, hcut, method = "wss")

print("Cut trees into 4 subgroups.")
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 20, fig.width = 30}
# Cut trees
sub_grp <- cutree(hc2, k = 4)
print("Number of members in each cluster:")
table(sub_grp)

plot(hc2, cex = 1, hang = -1, main = "Cluster of diets (ward D2 method)", cex.main=3)
rect.hclust(hc2, k = 4, border = 2:5)
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Plot subgroups in 2D
factoextra::fviz_cluster(list(data = nut_trans_dietCluster, cluster = sub_grp), labelsize = 5, main = "Cluster of diets (ward D2 method)")
```

__Clustering of subjects based on diet variables__

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Dissimilarity matrix
d <- dist(nut_trans, method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )
# Plot the obtained dendrogram
# plot(as.dendrogram(hc1), cex = 2, hang = -1, main = "Cluster of subjects based on their diets (complete method)", horiz = TRUE)
plot(hc1, cex = 0.4, hang = -1, main = "Cluster of subjects baed on diets (complete method)")

{
  library(cluster)
  # methods to assess agglomerative coefficient, which measures the amount of clustering structure found
  m <- c( "average", "single", "complete", "ward")
  names(m) <- c( "average", "single", "complete", "ward")
  
  # function to compute coefficient
  ac <- function(x) {
    agnes(nut_trans, method = x)$ac
  }
  map_dbl(m, ac)
}

hc2 <- hclust(d, method = "ward.D2" )
# plot(as.dendrogram(hc2), cex = 2, hang = -1, main = "Cluster of subjects based on their diets (ward D2 method)", horiz = TRUE)
plot(hc2, cex = 0.4, hang = -1, main = "Cluster of subjects baed on diets (ward D2 method)")
```

Use ward.D2 method, and then cut trees.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Decide how many subgroups based on the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square) is minimized
# See thos post: https://uc-r.github.io/kmeans_clustering#silo
factoextra::fviz_nbclust(nut_trans, hcut, method = "wss")

print("Cut trees into 4 subgroups.")
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Cut trees
sub_grp <- cutree(hc2, k = 4)
print("Number of members in each cluster:")
table(sub_grp)

plot(hc2, cex = 0.4, hang = -1, main = "Cluster of subjects baed on diets (ward D2 method)")
rect.hclust(hc2, k = 4, border = 2:5)
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Plot subgroups in 2D
factoextra::fviz_cluster(list(data = nut_trans, cluster = sub_grp), labelsize = 5, main = "Cluster of subjects baed on diets (ward D2 method)")
```

## Diets - Metabolites

### PCA

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# rm(list = ls())
load(file = "/Users/qiyan/Dropbox/PARENTs/Input/MetDietPheno_129_trans.RData")
# Need to sort the data, important!
nut_rmMissing <- nut_rmMissing[order(rownames(nut_rmMissing)),]
nut_trans <- nut_trans[order(rownames(nut_trans)),]

# Use non-transformed data matrix, center and scale = T
pca <- prcomp(nut_rmMissing, center = T, scale. = T)

screeplot(pca, type = "l", npcs = 15, main = "Screeplot of the first 10 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)
cumpro <- cumsum(pca$sdev^2 / sum(pca$sdev^2))
plot(cumpro[0:15], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(h = 0.8, col="blue", lty=5)

pca_score <- data.frame(pca$x[,1:10])

ggbiplot::ggbiplot(pca, ellipse=FALSE, var.axes = F) +
  labs(title = "PCA plot of diets")
```

Plot the loadings of PCs.

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 43, fig.width = 12}
pca_loading <- pca$rotation

colors <- rev(colorRampPalette(RColorBrewer::brewer.pal(10, "RdBu"))(256))
pheatmap::pheatmap(mat = pca_loading[,1:10], fontsize=12, fontsize_row=12, color = colors, cluster_cols = F, cellwidth = 25, cellheight = 12, main = "Loadings"
)
```

#### Correlation test: PC scores with metabolites

Only 93 subjects have all three datasets!

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
met_diet_input <- met_trans[which(met_trans$Subject_ID %in% rownames(nut_trans)),]
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
corr <- met_diet_input[,-1]

corr_summary <- data.frame(colnames(corr), PC1 = 1, PC2 = 1, PC3 = 1, PC4 = 1, PC5 = 1, PC6 = 1, PC7 = 1, PC8 = 1, PC9 = 1, PC10 = 1)

for (i in 1:10) {
  for (j in c(1:ncol(corr))){
    corr_summary[j,i+1] <- cor.test(corr[,j], pca_score[,i], na.action = na.rm)$p.value
  }
}

significant_variable_num <- {}
for (i in 1:nrow(corr_summary)) {
  if(sum(corr_summary[i,] < 0.05) > 0){
    temp <- corr_summary[i,1]
    significant_variable_num <- cbind(significant_variable_num, temp)
  }
}

print("Metabolites significanly associated with PC1 - PC10")
as.vector(significant_variable_num)
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 8}
num_corr <- cor(corr, pca_score,use = "p")

colors <- rev(colorRampPalette(RColorBrewer::brewer.pal(10, "RdBu"))(256))
pheatmap::pheatmap(mat = num_corr, fontsize=12, fontsize_row=12, color = colors, cluster_cols = F, main = "Correlation between metabolites and diet PCs")
```

### DietMWAS

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
met_diet_input <- met_diet_input[,-1]

DietMWAS_summary <- data.frame(); k=1

for (i in 1:ncol(met_diet_input)) {
  for (j in 1:ncol(nut_trans)) {
    temp <- summary(lm(met_diet_input[,i] ~ nut_trans[,j]))$coefficients[-1,4]
    if(sum(temp < 0.05) > 0){
      DietMWAS_summary[k,1] <- colnames(nut_trans)[j]
      DietMWAS_summary[k,2] <- colnames(met_diet_input)[i]
      k = k + 1
    }
  }
}

colnames(DietMWAS_summary) <- c("Diet", "Metabolite")

DietMWAS_summary <- merge(x = DietMWAS_summary, y = met_class[,c(1,3)], by.x = "Metabolite", by.y = "Met_ID", all.x = T)
DietMWAS_summary <- DietMWAS_summary[order(DietMWAS_summary$Diet),]
DietMWAS_summary <- DietMWAS_summary[,c("Diet", "Metabolite", "Class")]

# Calculate the correlation matrix
met_diet_corr <- cor(met_diet_input, nut_trans)

# Add correlation coeficient to dietMWAS_summary
for (i in 1:nrow(DietMWAS_summary)) {
  DietMWAS_summary$Beta[i] <- met_diet_corr[which(rownames(met_diet_corr) == DietMWAS_summary[i,2]), which(colnames(met_diet_corr) == DietMWAS_summary[i,1])]
}

DietMWAS_summary
```

```{r , echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.height = 54, fig.width = 20}
colors <- rev(colorRampPalette(RColorBrewer::brewer.pal(10, "RdBu"))(256))
pheatmap::pheatmap(mat = t(met_diet_corr), fontsize=12, fontsize_row=12, color = colors, cellwidth = 14, cellheight = 14, main = "Correlation matrix")
```

### Targeted diets - metabolties

```{r , echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
nut_trans <- nut_trans[order(rownames(nut_trans)),]
met_diet_input <- met_diet_input[order(rownames(met_diet_input)),]
combine_met_diet <- cbind(nut_trans, met_diet_input)

targeted_list <- read.xlsx(file = "/Users/qiyan/Dropbox/PARENTs/Raw Data/dietcalc_results_20190621.results_IDR.xlsx", sheetName = "Sheet4", header = T)

scatter_plot <- function(met, diet){
  sp <- ggscatter(combine_met_diet, x = met, y = diet,
   add = "reg.line",  # Add regressin line
   add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
   conf.int = TRUE # Add confidence interval
   )
# Add correlation coefficient
return(sp + stat_cor(method = "pearson"))
}

pdf(file = "/Users/qiyan/Dropbox/PARENTs/Output/Targeted_diet_met_correlation.pdf")
for (i in 1:nrow(targeted_list)) {
  met = targeted_list$Met[i]
  diet = targeted_list$Diet[i]
  print(scatter_plot(met = met, diet = diet))
}
dev.off()
```

Also try to use the raw metabolomcis and diet data.

```{r , echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
nut_rmMissing <- nut_rmMissing[order(rownames(nut_rmMissing)),]
met_raw_wDiet <- met[which(met$Subject_ID %in% rownames(nut_rmMissing)), -1]

met_raw_wDiet <- met_raw_wDiet[order(rownames(met_raw_wDiet)),]
combine_met_diet <- cbind(nut_rmMissing, met_raw_wDiet)

targeted_list <- read.xlsx(file = "/Users/qiyan/Dropbox/PARENTs/Raw Data/dietcalc_results_20190621.results_IDR.xlsx", sheetName = "Sheet4", header = T)

scatter_plot <- function(met, diet){
  sp <- ggscatter(combine_met_diet, x = met, y = diet,
   add = "reg.line",  # Add regressin line
   add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
   conf.int = TRUE # Add confidence interval
   )
# Add correlation coefficient
return(sp + stat_cor(method = "pearson"))
}

pdf(file = "/Users/qiyan/Dropbox/PARENTs/Output/Targeted_diet_met_correlation_raw.pdf")
for (i in 1:nrow(targeted_list)) {
  met = targeted_list$Met[i]
  diet = targeted_list$Diet[i]
  print(scatter_plot(met = met, diet = diet))
}
dev.off()
```



# To do

* Don't have Age and Sex for everyone...
* How to deal with missing values? Impute or zero, different variables need different imputation methods.
* How to deal with outliers? Threshold of outlier?
* Now we have both food composition and supplemental nutrients. Way to combine it.
* Develop indexes. Many countries use a food-based approach to dietary guidelines, so examining how the diet compares to dietary guidelines may be of interest to researchers and nutrition policy makers. Diet quality indexes (DQIs) are also widely used and are able to quantify the risk of some nutrition-related health outcomes. See https://www.sciencedirect.com/science/article/pii/B9780128029282000084.
* Pattern is more useful? See https://www.sciencedirect.com/science/article/pii/S2213858716304193, http://clinchem.aaccjnls.org/content/clinchem/64/1/82.full.pdf.





